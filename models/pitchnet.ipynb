{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ydb3Im3g40k"
   },
   "source": [
    "# IPYNB controler\n",
    "\n",
    "We provide few switch that activate/desactivate part of the IPYNB notebook.\n",
    "This control are usefull when the notebook is run by a shell command, or when using the FULL run button of Jupyter/Colaboratory.\n",
    "\n",
    "---\n",
    "\n",
    "To run this notebook in background, you can use\n",
    "```python\n",
    "runipy mynotebook.ipybn\n",
    "``` \n",
    "and\n",
    "```python\n",
    "jupyter nbconvert --to notebook --execute mynotebook.ipynb --output mynotebook.ipynb\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JDZ8FrOZiBEw"
   },
   "outputs": [],
   "source": [
    "build_network = False # Is the notebook run by the network building script ?\n",
    "activate_tensorboard = False # Show tensorboard in the cell\n",
    "prepare_dataset = True # Should we prepare the datasets if they do not exsits\n",
    "erase_present_datasets = False # Should we erase and download datasets again ?\n",
    "install_with_pip = False # Should we install dependecies with pip ?\n",
    "update_git = True # Should we update the ssh keys and git repo ?\n",
    "reduce_dataset_size = False # Should we reduce dataset size for debug ?\n",
    "show_graphs = True # Should we display some of the data as pyplot graphs ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L3p9xuAnTSyL"
   },
   "source": [
    "# Environment setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\" # 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up image display for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QI0DX9Ovb3u7"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Make pyplots BIGGERS!\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [12, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TNnLrrpx4uqR"
   },
   "outputs": [],
   "source": [
    "if activate_tensorboard:\n",
    "  if not build_network:\n",
    "    !yes | pip3 uninstall tb-nightly tensorboardX tensorboard\n",
    "    !pip3 install tensorboard pytorch-lightning\n",
    "    \n",
    "    !pkill tensorboard\n",
    "    !yes | wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
    "    #!yes | unzip ngrok-stable-linux-amd64.zip\n",
    "    LOG_DIR = \"./runs\"\n",
    "    get_ipython().system_raw(\n",
    "      'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
    "      .format(LOG_DIR)\n",
    "     )\n",
    "    #get_ipython().system_raw('./ngrok http 6006 &')\n",
    "    #! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
    "    #  \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\"\n",
    "    ! hostname -I | awk '{print \"http://\"$1\":6006\"}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l9n07y5AFssQ"
   },
   "source": [
    "##### Download datasets\n",
    "Download the 3 datasets if not already available on this machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uh0GrwQ5FsDL"
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "# Configure dataset names and directory\n",
    "voice_dataset = \"MonophonicVoiceDataset\"\n",
    "sampled_dataset = \"MonophonicSampleBasedDataset\"\n",
    "synth_dataset = \"MonophonicSynthDataset\"\n",
    "voice_dataset_directory = \"../\" + voice_dataset\n",
    "sampled_dataset_directory = \"../\" + sampled_dataset\n",
    "synth_dataset_directory = \"../\" + synth_dataset\n",
    "\n",
    "# Set the build_id to retrieve datasets\n",
    "build_id = \"20191226-d5d7e5763e7cf589fd2c21aafe82f7223cce48a8-6bebfdcd32c948860969c251c36e0ab6d26d6b89\"\n",
    "#build_id = \"20191227-e6c7f7f42cda7f8f2ab053b4a06c7d52c4d68c9e-6bebfdcd32c948860969c251c36e0ab6d26d6b89\"\n",
    "\n",
    "\n",
    "if prepare_dataset:\n",
    "  dataset_list = [voice_dataset, sampled_dataset, synth_dataset]\n",
    "\n",
    "  if erase_present_datasets:\n",
    "    for dataset_name in dataset_list:\n",
    "      !rm -rf $dataset_name $dataset_name\".zip\"\n",
    "\n",
    "  for dataset_name in dataset_list:\n",
    "    if not os.path.isdir(\"../\" + dataset_name):\n",
    "      if not os.path.isfile(\"../\" + dataset_name + \".zip\"):\n",
    "        print(\"No \", dataset_name + \".zip\")\n",
    "        !cp \"../dataset_builds/\"$dataset_name\"-\"$build_id\".zip\" \"../\"$dataset_name\".zip\"\n",
    "      !yes | unzip \"../\"$dataset_name\".zip\" -d \"../\"$dataset_name\n",
    "    else:\n",
    "      print(f\"{dataset_name} already available :)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n0K5iXnJp7o9"
   },
   "source": [
    "Import and install required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TD0weRjdb8mY"
   },
   "outputs": [],
   "source": [
    "if install_with_pip:\n",
    "    !python3 -m pip install Cython\n",
    "    !python3 -m pip install torchinfo pytorch-lightning \"torchvision>=0.4\" \"torch>=1.4\"\n",
    "    !yes | sudo aptitude install libsndfile1-dev\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as fn\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "# Import path\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "# Import pitchnet\n",
    "import pitchnet\n",
    "from pitchnet import *\n",
    "from pitchnet.dataset import *\n",
    "from pitchnet.io import *\n",
    "from pitchnet.model import *\n",
    "from pitchnet.preprocess import *\n",
    "from pitchnet.visualization import *\n",
    "\n",
    "# Allow auto-reload of the pitchnet library at each cell\n",
    "%reload_ext autoreload\n",
    "# %autoreload 1\n",
    "# %aimport pitchnet\n",
    "# %aimport pitchnet.io\n",
    "%autoreload 2\n",
    "\n",
    "# Torch version:\n",
    "print(\"torch.__version__ = \", torch.__version__)\n",
    "print(\"Cuda = \", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sef987fwocQp"
   },
   "source": [
    "# Create and display dataset\n",
    "\n",
    "The dataset contains 4 layers of informations (called frames) as input.\n",
    "The output is the segmentation followed by a one hot vector of probability of the given midi number. The dimension is Batch x Time x 1+128 = 129.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GzWmFPZJcsZK"
   },
   "outputs": [],
   "source": [
    "def split_dataset(dataset, ratio):\n",
    "    \"\"\"\n",
    "    Split a dataset into two parts.\n",
    "    \"\"\"\n",
    "    from torch.utils.data import Subset\n",
    "\n",
    "    split_index = int(ratio * len(dataset))\n",
    "    range_a = range(len(dataset))[:split_index]\n",
    "    range_b = range(len(dataset))[split_index:]\n",
    "    return Subset(dataset, range_a), Subset(dataset, range_b)\n",
    "\n",
    "#Datasets\n",
    "synth_dataset = import_dataset(synth_dataset_directory)\n",
    "voice_dataset = import_dataset(voice_dataset_directory)\n",
    "sampled_dataset = import_dataset(sampled_dataset_directory)\n",
    "\n",
    "# Truncate the length to the original 3000\n",
    "synth_dataset.length = 3000\n",
    "\n",
    "# Reduce size for faster training (debug):\n",
    "if reduce_dataset_size:\n",
    "    voice_dataset, _   = split_dataset(voice_dataset, 0.2)\n",
    "    sampled_dataset, _ = split_dataset(sampled_dataset, 0.2)\n",
    "    synth_dataset, _   = split_dataset(synth_dataset, 0.2)\n",
    "\n",
    "# Merge the datasets into a single one\n",
    "def merge_datasets(dt_list, separated_test=True):\n",
    "    \"\"\"\n",
    "    Merge a list of datasets into a single dataset\n",
    "    \"\"\"\n",
    "    val = []\n",
    "    train = []\n",
    "    test_list=[]\n",
    "    # Filter train and val set for each dataset\n",
    "    for dt in dt_list:\n",
    "        training_dt, validation_dt = split_dataset(dt, 0.5)\n",
    "        validation_dt, test_dt = split_dataset(validation_dt, 0.5)\n",
    "        val += [validation_dt]\n",
    "        train += [training_dt]\n",
    "        test_list += [test_dt]\n",
    "    # Glue dataset together\n",
    "    train = torch.utils.data.ConcatDataset(train)\n",
    "    test = torch.utils.data.ConcatDataset(test_list)\n",
    "    val = torch.utils.data.ConcatDataset(val)\n",
    "    \n",
    "    if separated_test:\n",
    "        return train, val, test, test_list\n",
    "    else:\n",
    "        return train, val, test\n",
    "\n",
    "training_set, validation_set, test_set, test_list = merge_datasets(\n",
    "    [synth_dataset, voice_dataset, sampled_dataset]\n",
    ")\n",
    "test_synth, test_voice, test_sampled = test_list\n",
    "\n",
    "\n",
    "# Generators\n",
    "params = {'batch_size': 16,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 6}\n",
    "\n",
    "training_generator = DataLoader(training_set, **params)\n",
    "validation_generator = DataLoader(validation_set, **{**params, 'shuffle': False})\n",
    "\n",
    "print(\"Training set length:\", len(training_set))\n",
    "print(\"Validation set length:\", len(validation_set))\n",
    "print(\"Test set length:\", len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(synth_dataset), len(voice_dataset), len(sampled_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SMC-_ES_7N7J",
    "tags": []
   },
   "source": [
    "## Visualize data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "11gWC5LOAF0c"
   },
   "source": [
    "Display the 3 channels of the input, and the expected midi notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image_data(xy):\n",
    "  x, y = xy\n",
    "  x = np.array(x)\n",
    "  y = np.array(y)\n",
    "  \n",
    "  # Outputs\n",
    "  segmentation_width = y[:, 0] # shape = T\n",
    "  segmentation_offset = y[:, 1] # shape = T\n",
    "  pitch = y[:, 2:] # shape = Tx128\n",
    "  pitch = list(map(onehot_to_pitch, pitch)) # shape = T\n",
    "  \n",
    "  # x shape is 3xTxS, we reshape to Tx3xS\n",
    "  plt.rcParams['figure.figsize'] = [15, 8]\n",
    "  def normalize(x):\n",
    "    from pitchnet.model import NormalizeCTS\n",
    "    return NormalizeCTS()(torch.Tensor(x)[None]).numpy()[0]\n",
    "  visualize_frames(signal=pitch, frames=normalize(x.transpose(1, 0, 2)), min_norm_amplitude=1, apply_log=False)\n",
    "\n",
    "def show_batch_data(dataset, number=2, seed=22):\n",
    "  np.random.seed(seed)\n",
    "  rand_idx = [np.random.randint(len(dataset)) for _ in range(number)]\n",
    "  for i in rand_idx:\n",
    "      xy = dataset.__getitem__(i)\n",
    "      show_image_data(xy, show=False)\n",
    "\n",
    "def show_batch_data_paper():\n",
    "    print(\"Synth\")\n",
    "    xy = synth_dataset.__getitem__(0)\n",
    "    show_image_data(xy)\n",
    "    print(\"Voice\")\n",
    "    xy = voice_dataset.__getitem__(0)\n",
    "    show_image_data(xy)\n",
    "    print(\"Sample Based\")\n",
    "    xy = sampled_dataset.__getitem__(0)\n",
    "    show_image_data(xy)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "if show_graphs:\n",
    "  # show_batch_data(voice_dataset, number=4, seed=7)\n",
    "    show_batch_data_paper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XfTt9TkM7_3i"
   },
   "source": [
    "# Create model\n",
    "\n",
    "Create a custom `model` inspired from resnet. The blocks are defined in the `pitchnet` library.\n",
    "\n",
    "The model does NOT output the exact same format as the dataset!\n",
    "\n",
    "* The output of the network is a LogProbability, whereas the output of dataset is a probability.\n",
    "* The model output a confidence and a presence factor that aren't in the target data (since fast to compute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m6s9SRP8p1DX"
   },
   "outputs": [],
   "source": [
    "# Compute dimensions (with batch first)\n",
    "inshape = [16] + list(training_set.__getitem__(0)[0].shape)\n",
    "print(\"Input  shape:\", inshape)\n",
    "\n",
    "# Free as much memory from the graphic card as possible\n",
    "import gc\n",
    "gc.collect();\n",
    "\n",
    "# Build model\n",
    "from pitchnet.model import build_model\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = build_model(ablation_autocorrelation=True).to(device)\n",
    "summary(model, inshape, depth=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x6ajZlrpx7ca"
   },
   "source": [
    "### Loss function:\n",
    "\n",
    "The loss takes as pitch a log probability (the prediction) and a probability or one hot vector (the target).\n",
    "\n",
    "It computes the kullback leibler distance between the distributions and mse of segmentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9vwkDLWNs4Y-"
   },
   "outputs": [],
   "source": [
    "from pitchnet.loss import *\n",
    "\n",
    "# Debug of the loss function:\n",
    "# (We have a look at the value of loss function between two uncorelated samples)\n",
    "y1 = training_set.__getitem__(0)[1][None,]\n",
    "y2 = training_set.__getitem__(3)[1][None,]\n",
    "\n",
    "print(\"Loss between two samples:\", loss_fn(y1, y2)[0].item())\n",
    "del y1\n",
    "del y2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wrVkA_xlynAW"
   },
   "source": [
    "# Model Training\n",
    "\n",
    "First, we write a function that display graphs and curves to monitor the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e5qg1HHMLEFh"
   },
   "source": [
    "The following function allow to log the layers of the actual model into tensorboard.\n",
    "\n",
    "Convolutional network weigths are turned into pictures. Both the distribution of weights and bias are also available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dfc-VNCi7EDc"
   },
   "outputs": [],
   "source": [
    "def log_model(model, tensorboard, step=0):\n",
    "    body = model[2]\n",
    "    head = model[3]\n",
    "  \n",
    "  # Normalise images for better visualization\n",
    "    def norm(patchs):\n",
    "      patchs = torch.abs(patchs)\n",
    "      max_val = patchs.max(dim=3, keepdim=True)[0].max(dim=2, keepdim=True)[0]\n",
    "      patchs = patchs / max_val\n",
    "      return patchs\n",
    "  \n",
    "    # Log a 2d conv layer\n",
    "    def log_conv2d(summarywritter, conv, name, idx=0):\n",
    "        conv = conv.weight\n",
    "        c_out, c_in, w, h = conv.shape\n",
    "        # Log kernels as images\n",
    "        if c_in == 3:\n",
    "            patchs = conv.view(c_out, c_in, w, h)\n",
    "            summarywritter.add_images(name + \"_%d/Patches_RED\"   % idx, norm(patchs[:,0:1,:,:]), global_step=step, dataformats='NCWH')\n",
    "            summarywritter.add_images(name + \"_%d/Patches_GREEN\" % idx, norm(patchs[:,1:2,:,:]), global_step=step, dataformats='NCWH')\n",
    "            summarywritter.add_images(name + \"_%d/Patches_BLUE\"  % idx, norm(patchs[:,2:3,:,:]), global_step=step, dataformats='NCWH')\n",
    "            summarywritter.add_images(name + \"_%d/Patches_RGB\"  % idx, norm(patchs), global_step=step, dataformats='NCWH')\n",
    "        elif w == 1:\n",
    "            patchs = conv.view(c_out, 1, c_in, h)\n",
    "            summarywritter.add_images(name + \"_%d/Patches\" % idx, norm(patchs), global_step=step, dataformats='NCWH')\n",
    "        else:\n",
    "            patchs = conv.view(c_out * c_in, 1, w, h)\n",
    "            summarywritter.add_images(name + \"_%d/Patches\" % idx, norm(patchs), global_step=step, dataformats='NCWH')\n",
    "        # Log kernels distributions\n",
    "        vector = patchs.view(-1)\n",
    "        summarywritter.add_histogram(name + \"_%d/Weight_distribution\" % idx, vector, global_step=step)\n",
    "        # Log bias distributions\n",
    "        try:\n",
    "            vector = conv.bias.view(-1)\n",
    "            summarywritter.add_histogram(name + \"_%d/Bias_distribution\" % idx, vector, global_step=step)\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "  \n",
    "    # Log a 1d conv layer\n",
    "    def log_conv1d(summarywritter, layer, name=\"model\"):\n",
    "        conv = layer.weight\n",
    "        c_out, c_in, n = conv.shape\n",
    "        print(conv.permute(2, 0, 1).shape)\n",
    "        # Log kernels as images\n",
    "        patchs = conv.permute(2, 0, 1)[:, None, :, :]\n",
    "        summarywritter.add_image(name + \"/Patches\", norm(patchs), global_step=step, dataformats='NCWH')\n",
    "        # Log kernels distribution\n",
    "        vector = patchs.reshape(-1)\n",
    "        summarywritter.add_histogram(name + \"/Weight_distribution\", vector, global_step=step)\n",
    "        # Log bias if available\n",
    "        try:\n",
    "            vector = layer.bias.view(-1)\n",
    "            summarywritter.add_histogram(name + \"/Bias_distribution\", vector, global_step=step)\n",
    "        except AttributeError:\n",
    "            pass\n",
    "  \n",
    "    # Indexes of layer that we will meet\n",
    "    indexes = {\n",
    "        \"bottleneck_idx\": 0,\n",
    "        \"conv_idx\": 0,\n",
    "        \"dilation_idx\": 0\n",
    "    }\n",
    "  \n",
    "    def log_module(self, indexes=indexes):\n",
    "        global conv_idx, bottleneck_idx, dilation_idx\n",
    "        # Log current module\n",
    "        if type(self) == BottleneckBlock:\n",
    "            indexes[\"bottleneck_idx\"] += 1\n",
    "            log_conv2d(tensorboard, self.layer[0], \"BottleneckBlock\", indexes[\"bottleneck_idx\"])\n",
    "\n",
    "        if type(self) == nn.Conv2d:\n",
    "            indexes[\"conv_idx\"] += 1\n",
    "            log_conv2d(tensorboard, self, \"Conv2d\", indexes[\"conv_idx\"])\n",
    "\n",
    "        if type(self) == nn.Conv1d:\n",
    "            indexes[\"conv_idx\"] += 1\n",
    "            log_conv1d(tensorboard, self, \"Conv1d\", indexes[\"conv_idx\"])\n",
    "\n",
    "        # Log sub modules\n",
    "        for layer in self.children():\n",
    "            log_module(layer, indexes)\n",
    "    \n",
    "    # Log body of model\n",
    "    body.apply(log_module)\n",
    "  \n",
    "    # Helper to get something close to square root\n",
    "    def factor_int(n):\n",
    "        nsqrt = math.ceil(math.sqrt(n))\n",
    "        solution = False\n",
    "        val = nsqrt\n",
    "        while not solution:\n",
    "            val2 = int(n/val)\n",
    "            if val2 * val == float(n):\n",
    "                solution = True\n",
    "            else:\n",
    "                val-=1\n",
    "        return int(val), int(val2), n\n",
    "  \n",
    "    #TODO LOG\n",
    "    log_conv1d(tensorboard, head.pitch, name=\"Head\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "urhba5QUyZvC"
   },
   "outputs": [],
   "source": [
    "class PitchnetModule(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, model, loss_fn):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.lr = 1e-3\n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        # self.device = torch.device(\"cuda:0\" if self.use_cuda else \"cpu\")\n",
    "        self.to(torch.device(\"cuda:0\" if self.use_cuda else \"cpu\"))\n",
    "\n",
    "\n",
    "        # Move on the right device\n",
    "        self.to(self.device)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model.forward(x.to(self.device))\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        import gc\n",
    "        gc.collect()\n",
    "        # Log model internals\n",
    "        if batch_idx % 10 == 0:\n",
    "            display_hook(pitchnet_module, logger=self.logger)\n",
    "\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss, pitch_loss, seg_width_loss, seg_offset_loss, seg_confidence_loss, seg_presence_loss, iou_loss, onset_loss = self.loss_fn(y_hat, y.to(self.device))\n",
    "        tensorboard_logs = {\n",
    "            'train_loss': loss,\n",
    "            'train_pitch_loss': pitch_loss,\n",
    "            'train_seg_width_loss': seg_width_loss,\n",
    "            'train_seg_offset_loss': seg_offset_loss,\n",
    "            'train_seg_confidence_loss': seg_confidence_loss,\n",
    "            'train_seg_presence_loss': seg_presence_loss,\n",
    "            'train_iou_loss': iou_loss,\n",
    "            'train_onset_loss': onset_loss,\n",
    "            }\n",
    "        self.log_dict(tensorboard_logs)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss, pitch_loss, seg_width_loss, seg_offset_loss, seg_confidence_loss, seg_presence_loss, iou_loss, onset_loss = self.loss_fn(y_hat, y.to(self.device))\n",
    "        tensorboard_logs = {\n",
    "            'val_loss': loss,\n",
    "            'val_pitch_loss': pitch_loss,\n",
    "            'val_seg_width_loss': seg_width_loss,\n",
    "            'val_seg_offset_loss': seg_offset_loss,\n",
    "            'val_seg_confidence_loss': seg_confidence_loss,\n",
    "            'val_seg_presence_loss': seg_presence_loss,\n",
    "            'val_iou_loss': iou_loss,\n",
    "            'val_onset_loss': onset_loss,\n",
    "            }\n",
    "        self.log_dict(tensorboard_logs)\n",
    "        return loss\n",
    "\n",
    "    def validation_end(self, outputs):\n",
    "        # Log validation mean\n",
    "        val_loss_mean = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        self.log_dict({'val_loss_mean': val_loss_mean})\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # can return multiple optimizers and learning_rate schedulers\n",
    "        # (LBFGS it is automatically supported, no need for closure function)\n",
    "        return self.optimizer\n",
    "\n",
    "    # @pl.data_loader\n",
    "    def train_dataloader(self):\n",
    "        return training_generator\n",
    "\n",
    "    # @pl.data_loader\n",
    "    def val_dataloader(self):\n",
    "        return validation_generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ykcfkeNkFt4t"
   },
   "outputs": [],
   "source": [
    "def output_to_pitch(produced):\n",
    "    return np.array([onehot_to_pitch(np.exp(v)) for v in produced[:, -128:]])\n",
    "def label_to_pitch(expected):\n",
    "    return np.array([onehot_to_pitch(v) for v in expected[:, -128:]])\n",
    "\n",
    "def display_hook(module, logger):\n",
    "    #TODO: rewrite\n",
    "    trainer = module\n",
    "    # Change PLT size\n",
    "    prev_plt_size = plt.rcParams['figure.figsize']\n",
    "    plt.rcParams['figure.figsize'] = [14, 15]\n",
    "\n",
    "    # Select a radom sample from training set\n",
    "    import random\n",
    "    #x, y = training_set.__getitem__(random.randint(0,len(training_set)-1))\n",
    "    x, y = validation_set.__getitem__(random.randint(0,len(validation_set)-1))\n",
    "    # DEBUG: Display only on sampled dataset\n",
    "    #x, y = sampled_dataset.__getitem__(random.randint(0,len(sampled_dataset)-1))\n",
    "\n",
    "    # Evaluate with forward pass\n",
    "    with torch.set_grad_enabled(False):\n",
    "        out = trainer.model(x[None, :, :, :].to(trainer.device))\n",
    "\n",
    "    # Display expected/evaluated curves for both outputs (midi/segmentation)\n",
    "    produced = np.array(out.cpu().detach())[0]\n",
    "    expected = np.array(y.cpu().detach())\n",
    "\n",
    "    def centernorm(x):\n",
    "        #x = (x - x.mean())\n",
    "        return x / np.abs(x).max()\n",
    "\n",
    "\n",
    "    # Display Segmentation\n",
    "    fig = plt.figure()\n",
    "    ax = plt.subplot(511)\n",
    "    ax.title.set_text('Seg Width')\n",
    "    plt.plot(expected[:, 0], color=\"green\")\n",
    "    plt.plot(produced[:, 0], color=\"blue\")\n",
    "\n",
    "    # Displat offset\n",
    "    ax = plt.subplot(512)\n",
    "    ax.title.set_text('Seg Offset')\n",
    "    plt.plot(expected[:, 1], color=\"green\")\n",
    "    plt.plot((produced[:, 1]) * 0.5 + 0.5, color=\"blue\")\n",
    "    plt.plot(produced[:, 3], color=\"red\")\n",
    "\n",
    "    # Displat Pitch\n",
    "    ax = plt.subplot(513)\n",
    "    ax.title.set_text('Pitch')\n",
    "    plt.plot(label_to_pitch(expected), color=\"blue\")\n",
    "    plt.plot(output_to_pitch(produced), color=\"red\")\n",
    "\n",
    "    plt.subplot(514)\n",
    "    plt.imshow(produced[:, -128:].T, origin='lower', aspect='auto')\n",
    "\n",
    "    # Losses\n",
    "    ax = plt.subplot(515)\n",
    "    ax.title.set_text('IOU')\n",
    "    p2 = (expected[:, 0] != 0) * 1.0 \n",
    "    seg1_width, seg1_offset = produced[:, 0], produced[:, 1]\n",
    "    seg2_width, seg2_offset = expected[:, 0], expected[:, 1]\n",
    "    c_pitch1 = onehot_to_pitch_torch2(torch.exp(torch.Tensor(produced[:, -128:]))) * p2\n",
    "    c_pitch1 = c_pitch1.numpy()\n",
    "    c_pitch2 = onehot_to_pitch_torch2(torch.Tensor(expected[:, -128:])) * p2\n",
    "    c_pitch2 = c_pitch2.numpy()\n",
    "    iou = calculate_iou(seg1_width, seg1_offset * 0.5 + 0.5, c_pitch1, seg2_width, seg2_offset, c_pitch2)\n",
    "    # deltap = (c_pitch1 - c_pitch2).abs().numpy()\n",
    "    plt.plot(iou, label='iou')\n",
    "    plt.plot(produced[:, 2], color=\"orange\")\n",
    "    ax.legend()\n",
    "    \n",
    "    try:\n",
    "        x_train = np.linspace(0, len(trainer.train_losses), len(trainer.train_losses))\n",
    "        x_valid = np.linspace(0, len(trainer.train_losses), len(trainer.valid_losses))\n",
    "        plt.plot(x_train, trainer.train_losses)\n",
    "        plt.plot(x_valid, trainer.valid_losses)\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    logger.experiment.add_figure(\"Display/Validation\", fig, display_hook.iter)\n",
    "    log_model(trainer.model, logger.experiment, display_hook.iter)\n",
    "    display_hook.iter +=1\n",
    "\n",
    "    # Restore PLT size\n",
    "    plt.rcParams['figure.figsize'] = prev_plt_size\n",
    "\n",
    "try:\n",
    "    display_hook.iter\n",
    "except AttributeError:\n",
    "    display_hook.iter = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TDdyuwOpUZJ7"
   },
   "source": [
    "We now train the network while monitoring and loging the losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TlZ7Rem64BMu"
   },
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "import gc\n",
    "# try:\n",
    "#   pitchnet\n",
    "# except NameError:\n",
    "pitchnet_module = PitchnetModule(model, loss_fn)\n",
    "# pitchnet_module = PitchnetModule.load_from_checkpoint(\"runs/pitchnet-pitch+presence+dil3i2-nopresence/version_1/checkpoints/epoch=131-step=17556.ckpt\", model=model, loss_fn=loss_fn)\n",
    "\n",
    "# Init logger\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "logger = TensorBoardLogger(\"runs\", name=\"pitchnet-pitchd23w2+3datasets+ablation_autocorrelation\")\n",
    "\n",
    "# Display model after initialization\n",
    "# print(\"Model before training:\")\n",
    "# display_hook(pitchnet_module, logger=logger)\n",
    "\n",
    "# Train neural net\n",
    "trainer = pl.Trainer(max_epochs=300,\n",
    "                     limit_train_batches=1.0, logger=logger,\n",
    "                     log_every_n_steps=1, accelerator='gpu',\n",
    "                     callbacks=[pl.callbacks.ModelCheckpoint(\n",
    "                        save_top_k=3,\n",
    "                        save_last=True,\n",
    "                        # mode=\"min\" if \"acc\" not in hparams.metric_to_track else \"max\",\n",
    "                        monitor='val_loss',\n",
    "                        dirpath=None,\n",
    "                        # filename=\"{epoch}\",\n",
    "                        verbose=True,\n",
    "                    )]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(pitchnet_module)\n",
    "\n",
    "logger.finalize(\"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls runs/pitchnet-pitchd23w2+3datasets+ablation_autocorrelation/version_0/checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp runs/pitchnet-pitchd23w2+3datasets+ablation_autocorrelation/version_0/checkpoints/epoch=20-step=4914.ckpt pitchnet-ablation-pitchd23w2+3datasets.ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp runs/pitchnet-pitchd23w2+3datasets/version_3/checkpoints/epoch=23-step=5616.ckpt pitchnet-pitchd23w2+3datasets.ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First good model:\n",
    "# pitchnet_module = pitchnet_module.load_from_checkpoint('runs/pitchnet-pitchd23w2+3datasets/version_3/checkpoints/epoch=23-step=5616.ckpt', model=model, loss_fn=loss_fn)\n",
    "\n",
    "pitchnet_module = pitchnet_module.load_from_checkpoint('runs/pitchnet-pitchd23w2+3datasets+ablation_autocorrelation/version_0/checkpoints/epoch=21-step=5148.ckpt', model=model, loss_fn=loss_fn)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-4oXeoX71vbo"
   },
   "outputs": [],
   "source": [
    "# Recover model\n",
    "model = pitchnet_module.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VpIHg4F5snY0"
   },
   "source": [
    "# Visualization of training result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n9Qtb7tN89qf"
   },
   "source": [
    "Display some nice graph of what happened during learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cQ7aoI2kIEYk"
   },
   "outputs": [],
   "source": [
    "# Show curve in plotly (allow zooming !!!)\n",
    "def show_curve_plotly(signal, title=None, **kwargs):\n",
    "  fig = go.Figure()\n",
    "  \n",
    "  if type(signal) == tuple:\n",
    "    for s in signal:\n",
    "      fig.add_trace(go.Scatter(x=list(range(len(s))), y=s, **kwargs))\n",
    "  else:\n",
    "    fig.add_trace(go.Scatter(x=list(range(len(signal))), y=signal, **kwargs))\n",
    "    \n",
    "  if title is not None:\n",
    "    fig.update_layout(\n",
    "      title=go.layout.Title(text=title,\n",
    "          xref=\"paper\",\n",
    "          x=0\n",
    "      )\n",
    "    )\n",
    "  fig.show()\n",
    "\n",
    "# Show curve in pyplot\n",
    "def show_curve(signal, title=None, **kwargs):\n",
    "    plt.figure()\n",
    "    if title: plt.title(title)\n",
    "  \n",
    "    if type(signal) == tuple:\n",
    "        for s in signal:\n",
    "            plt.plot(s)# ; plt.show()\n",
    "    else:\n",
    "        plt.plot(signal)# ; plt.show()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z_5LteAfrTQ1"
   },
   "source": [
    "# Export Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4JGlA__oI-uR"
   },
   "source": [
    "We first export the build_id of the datasets used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XaUpxQ6mJBuH"
   },
   "outputs": [],
   "source": [
    "with open(\"datasets_build_id\", 'w') as file:\n",
    "    file.write(build_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SwED39xQ8uQt"
   },
   "source": [
    "## Export in pytorch format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_DVLRPzHsHSF"
   },
   "outputs": [],
   "source": [
    "input_shape = (1, 500, 4, 513)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "__gvIxXnrukA"
   },
   "source": [
    "Export directly by saving the pytorch model with code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A6WKZhG38dFb"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "!mkdir -p models\n",
    "torch.save(model, \"models/monophonic_net.pt\")\n",
    "torch.save(model, \"models/monophonic_net_\"+ datetime.now().strftime(\"%Y_%m_%d-%H_%M_%S\") + \".pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nOilI-E0rzYS"
   },
   "source": [
    "Export after compiling using the JIT library. This allow loading/runing the model without any code from this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WD35wnKQu2CR"
   },
   "outputs": [],
   "source": [
    "# Compile network and save it\n",
    "timestr = datetime.now().strftime(\"%Y_%m_%d-%H_%M_%S\")\n",
    "name = 'models/monophonic_net_jit_' + timestr + '.pt'\n",
    "jit_model = torch.jit.trace(model.cpu(), torch.randn(input_shape))\n",
    "jit_model.save(name)\n",
    "del jit_model\n",
    "import gc\n",
    "gc.collect()\n",
    "model.cuda();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6UEnK20_rhbT"
   },
   "source": [
    "## Export to ONNX format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9pjc--sGAzcF"
   },
   "source": [
    "We start by patching the local ONNX version of python to take one that finaly support log_softmax. This should be removed when the standard pytorch version used contain this commit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wCfdmCwaAyxF"
   },
   "outputs": [],
   "source": [
    "# Download a working ONNX 9 opcode implementation\n",
    "!curl -O https://raw.githubusercontent.com/pytorch/pytorch/3ada2e0d64b40622e823b8135d2bbbc74e6526b9/torch/onnx/symbolic_opset9.py\n",
    "!cp symbolic_opset9.py /usr/local/lib/python3.6/dist-packages/torch/onnx/\n",
    "!sudo cp symbolic_opset9.py /opt/anaconda3/lib/python3.7/site-packages/torch/onnx/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may need to restart your python runtime\n",
    "import torch.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mM2C8FCrrjdw"
   },
   "outputs": [],
   "source": [
    "# Create dummy input\n",
    "dummy_input = torch.rand(input_shape).cuda()\n",
    "gc.collect()\n",
    "# Define input / output names\n",
    "input_names = [\"network_input\"]\n",
    "output_names = [\"network_output\"]\n",
    "\n",
    "# Convert the PyTorch model to ONNX\n",
    "timestr = datetime.now().strftime(\"%Y_%m_%d-%H_%M_%S\")\n",
    "onnx_filename = 'models/monophonic_net_' + timestr + '.onnx'\n",
    "torch.onnx.export(model,\n",
    "                  dummy_input,\n",
    "                  onnx_filename,\n",
    "                  verbose=True,\n",
    "                  input_names=input_names,\n",
    "                  output_names=output_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [12, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kP13qYjSGpp7"
   },
   "source": [
    "Display the trained model on a random sample from validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "94KEcbKD7m24"
   },
   "outputs": [],
   "source": [
    "import gc; gc.collect()\n",
    "\n",
    "# Select a radom sample\n",
    "import random\n",
    "x, y = validation_set.__getitem__(random.randint(0,len(validation_set) - 1))\n",
    "# x, y = voice_dataset.__getitem__(random.randint(0,len(validation_set) - 1))\n",
    "# x, y = voice_dataset.__getitem__(0)\n",
    "# x, y = synth_dataset.__getitem__(random.randint(0,len(validation_set) - 1))\n",
    "# x, y = synth_dataset.__getitem__(len(validation_set) - 1)\n",
    "\n",
    "# Evaluate with forward pass\n",
    "with torch.set_grad_enabled(False):\n",
    "    out = model(x[None, :, :, :].to(pitchnet_module.device))\n",
    "\n",
    "# Display expected/evaluated curves for both outputs (midi/segmentation)\n",
    "produced = np.array(out.cpu().detach())[0]\n",
    "expected = np.array(y.cpu().detach())\n",
    "\n",
    "# show_curve((expected[:, 0], produced[:, 0]), title='Width')\n",
    "# show_curve((expected[:, 1], produced[:, 1] * 0.5 + 0.5), title='Offset')\n",
    "# show_curve(((expected[:, 0] != 0) * 1.0, produced[:, 2]), title='Presence')\n",
    "# show_curve(([onehot_to_pitch(v) for v in expected[:, -128:]], [onehot_to_pitch(np.exp(v)) for v in produced[:, -128:]]), title='Pitch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppp = produced.copy()\n",
    "ppp[:, -128:] = np.exp(ppp[:, -128:])\n",
    "visualize_onehot_segmentation(ppp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8DKpQPAiSEv-"
   },
   "outputs": [],
   "source": [
    "visualize_onehot_segmentation(produced)\n",
    "visualize_onehot_segmentation(expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measure performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(model, batch):\n",
    "    x, y = batch\n",
    "    y_hat = model.forward(x)\n",
    "    loss, pitch_loss, seg_width_loss, seg_offset_loss, seg_confidence_loss, seg_presence_loss, iou_loss, onset_loss = self.loss_fn(y_hat, y.to(self.device))\n",
    "    tensorboard_logs = {\n",
    "        'val_loss': loss,\n",
    "        'val_pitch_loss': pitch_loss,\n",
    "        'val_seg_width_loss': seg_width_loss,\n",
    "        'val_seg_offset_loss': seg_offset_loss,\n",
    "        'val_seg_confidence_loss': seg_confidence_loss,\n",
    "        'val_seg_presence_loss': seg_presence_loss,\n",
    "        'val_iou_loss': iou_loss,\n",
    "        'val_onset_loss': onset_loss,\n",
    "        }\n",
    "    self.log_dict(tensorboard_logs)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitchnet_module.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_perf(dataset, batch_size=16, num_workers=8, accuracy_threshold=50, verbose=True):\n",
    "    \"\"\"\n",
    "    Accuracy threshold expressed in cents (100cents = 1 semi tone)\n",
    "    \"\"\"\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    \n",
    "    \n",
    "    def get_quantiles(error, weight):\n",
    "        error = torch.Tensor(error)\n",
    "        mean = error.mean()\n",
    "        q25, q75 = torch.quantile(error, 0.25), torch.quantile(torch.Tensor(error), 0.75)\n",
    "        q99 = torch.quantile(error, 0.99)\n",
    "        q50 = error.median()\n",
    "        return q25, q50, q75, q99,  mean\n",
    "    \n",
    "    \n",
    "    errors = []\n",
    "    errors_30ms = []\n",
    "    accuracies = []\n",
    "    accuracies_30ms = []\n",
    "    weights = []\n",
    "    for mini_batch in loader:\n",
    "        x, y = mini_batch\n",
    "        z = pitchnet_module(x)\n",
    "\n",
    "        pitch1 = z[:, :, -128:]\n",
    "        pitch2 = y[:, :, -128:]\n",
    "        pitch_exists = pitch2[:, :, 1:].sum(dim=2) > 0\n",
    "\n",
    "        pitch_estimated = onehot_to_pitch_torch2(torch.exp(pitch1)).cpu() * pitch_exists # NxT\n",
    "        pitch_label = onehot_to_pitch_torch2(pitch2).cpu() * pitch_exists # NxT\n",
    "        \n",
    "        # How much non zero information is contained in this batch\n",
    "        weight = pitch_exists.sum()\n",
    "        weights.append(weight.item())\n",
    "        nb_unused_cells = (~pitch_exists).sum()\n",
    "\n",
    "        # The error in pitch for non zero labels\n",
    "        error = (pitch_estimated - pitch_label).abs()\n",
    "        error_p1 = torch.zeros_like(error)\n",
    "        error_m1 = torch.zeros_like(error)\n",
    "        error_p1[:, 1:] = (pitch_estimated[:, 1:] - pitch_label[:, :-1]).abs()\n",
    "        error_m1[:, :-1] = (pitch_estimated[:, :-1] - pitch_label[:, 1:]).abs()\n",
    "        error_30ms = torch.minimum(error, torch.minimum(error_p1, error_m1))\n",
    "        errors.append(error[pitch_exists].numpy())\n",
    "        errors_30ms.append(error_30ms[pitch_exists].numpy())\n",
    "        \n",
    "        # The accuracy with a \n",
    "        accuracy_ar = error < 50 / 100\n",
    "        accuracy = (accuracy_ar.sum() - nb_unused_cells ) / weight\n",
    "        accuracies.append(accuracy.item())\n",
    "        \n",
    "        \n",
    "        accuracy_30ms = error_30ms < 50/100\n",
    "        accuracy_30ms = (accuracy_30ms.sum() - nb_unused_cells ) / weight\n",
    "        accuracies_30ms.append(accuracy_30ms.item())\n",
    "        \n",
    "        if verbose:\n",
    "            print('Accuracy 10ms:', accuracy.item(), 'and 30ms:', accuracy_30ms.item())\n",
    "        \n",
    "        #debug\n",
    "        # if len(accuracies) > 3:\n",
    "        #     break\n",
    "    \n",
    "    accuracies = np.array(accuracies)\n",
    "    weights = np.array(weights)\n",
    "    errors = np.concatenate(errors)\n",
    "    errors_30ms = np.concatenate(errors_30ms)\n",
    "    \n",
    "    # Total accuracy\n",
    "    total_accuracy = (accuracies * weights).sum() / weights.sum()\n",
    "    print(f\"Total accuracy: {total_accuracy}\")\n",
    "    \n",
    "    # Total accuracy 30ms\n",
    "    total_accuracy = (accuracies_30ms * weights).sum() / weights.sum()\n",
    "    print(f\"Total accuracy 30ms: {total_accuracy}\")\n",
    "    \n",
    "    # Total errors\n",
    "    q25, q50, q75, q99, mean = get_quantiles(errors.reshape(-1), weight.sum())\n",
    "    print(f\"Error in semitones: mean:{mean}, 25th:{q25}, median:{q50}, 75th:{q75}, 99th:{q99}\")\n",
    "    \n",
    "    \n",
    "    q25, q50, q75, q99, mean = get_quantiles(errors_30ms.reshape(-1), weight.sum())\n",
    "    print(f\"Error in semitones (30ms): mean:{mean}, 25th:{q25}, median:{q50}, 75th:{q75}, 99th:{q99}\")\n",
    "    \n",
    "    return errors, accuracy\n",
    "\n",
    "print(\"-\"*20)\n",
    "print(\"- Generate Report\")\n",
    "print(\"-\"*20)\n",
    "print(\"Synth\")\n",
    "evaluate_perf(test_synth, batch_size=16, num_workers=8)\n",
    "print(\"*\"*10)\n",
    "print(\"Voice\")\n",
    "evaluate_perf(test_voice, batch_size=16, num_workers=8)\n",
    "print(\"*\"*10)\n",
    "print(\"Sampled\")\n",
    "evaluate_perf(test_sampled, batch_size=16, num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_perf(dataset, batch_size=16, num_workers=8, accuracy_threshold=50, verbose=True):\n",
    "    \"\"\"\n",
    "    Accuracy threshold expressed in cents (100cents = 1 semi tone)\n",
    "    \"\"\"\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    \n",
    "    \n",
    "    def get_quantiles(error, weight):\n",
    "        error = torch.Tensor(error)\n",
    "        mean = error.mean()\n",
    "        q25, q75 = torch.quantile(error, 0.25), torch.quantile(torch.Tensor(error), 0.75)\n",
    "        q99 = torch.quantile(error, 0.99)\n",
    "        q50 = error.median()\n",
    "        return q25, q50, q75, q99,  mean\n",
    "    \n",
    "    \n",
    "    errors = []\n",
    "    errors_30ms = []\n",
    "    accuracies = []\n",
    "    accuracies_30ms = []\n",
    "    weights = []\n",
    "    for mini_batch in loader:\n",
    "        x, y = mini_batch\n",
    "        z = pitchnet_module(x)\n",
    "\n",
    "        pitch1 = z[:, :, -128:]\n",
    "        pitch2 = y[:, :, -128:]\n",
    "        pitch_exists = pitch2[:, :, 1:].sum(dim=2) > 0\n",
    "\n",
    "        pitch_estimated = onehot_to_pitch_torch2(torch.exp(pitch1)).cpu() * pitch_exists # NxT\n",
    "        pitch_label = onehot_to_pitch_torch2(pitch2).cpu() * pitch_exists # NxT\n",
    "        \n",
    "        # How much non zero information is contained in this batch\n",
    "        weight = pitch_exists.sum()\n",
    "        weights.append(weight.item())\n",
    "        nb_unused_cells = (~pitch_exists).sum()\n",
    "\n",
    "        # The error in pitch for non zero labels\n",
    "        error = (pitch_estimated - pitch_label).abs()\n",
    "        error_p1 = torch.zeros_like(error)\n",
    "        error_m1 = torch.zeros_like(error)\n",
    "        error_p1[:, 1:] = (pitch_estimated[:, 1:] - pitch_label[:, :-1]).abs()\n",
    "        error_m1[:, :-1] = (pitch_estimated[:, :-1] - pitch_label[:, 1:]).abs()\n",
    "        error_30ms = torch.minimum(error, torch.minimum(error_p1, error_m1))\n",
    "        errors.append(error[pitch_exists].numpy())\n",
    "        errors_30ms.append(error_30ms[pitch_exists].numpy())\n",
    "        \n",
    "        # The accuracy with a \n",
    "        accuracy_ar = error < 50 / 100\n",
    "        accuracy = (accuracy_ar.sum() - nb_unused_cells ) / weight\n",
    "        accuracies.append(accuracy.item())\n",
    "        \n",
    "        \n",
    "        accuracy_30ms = error_30ms < 50/100\n",
    "        accuracy_30ms = (accuracy_30ms.sum() - nb_unused_cells ) / weight\n",
    "        accuracies_30ms.append(accuracy_30ms.item())\n",
    "        \n",
    "        if verbose:\n",
    "            print('Accuracy 10ms:', accuracy.item(), 'and 30ms:', accuracy_30ms.item())\n",
    "        \n",
    "        #debug\n",
    "        # if len(accuracies) > 3:\n",
    "        #     break\n",
    "    \n",
    "    accuracies = np.array(accuracies)\n",
    "    weights = np.array(weights)\n",
    "    errors = np.concatenate(errors)\n",
    "    errors_30ms = np.concatenate(errors_30ms)\n",
    "    \n",
    "    # Total accuracy\n",
    "    total_accuracy = (accuracies * weights).sum() / weights.sum()\n",
    "    print(f\"Total accuracy: {total_accuracy}\")\n",
    "    \n",
    "    # Total accuracy 30ms\n",
    "    total_accuracy = (accuracies_30ms * weights).sum() / weights.sum()\n",
    "    print(f\"Total accuracy 30ms: {total_accuracy}\")\n",
    "    \n",
    "    # Total errors\n",
    "    q25, q50, q75, q99, mean = get_quantiles(errors.reshape(-1), weight.sum())\n",
    "    print(f\"Error in semitones: mean:{mean}, 25th:{q25}, median:{q50}, 75th:{q75}, 99th:{q99}\")\n",
    "    \n",
    "    \n",
    "    q25, q50, q75, q99, mean = get_quantiles(errors_30ms.reshape(-1), weight.sum())\n",
    "    print(f\"Error in semitones (30ms): mean:{mean}, 25th:{q25}, median:{q50}, 75th:{q75}, 99th:{q99}\")\n",
    "    \n",
    "    return errors, accuracy\n",
    "\n",
    "print(\"-\"*20)\n",
    "print(\"- Generate Report\")\n",
    "print(\"-\"*20)\n",
    "print(\"Synth\")\n",
    "evaluate_perf(test_synth, batch_size=16, num_workers=8)\n",
    "print(\"*\"*10)\n",
    "print(\"Voice\")\n",
    "evaluate_perf(test_voice, batch_size=16, num_workers=8)\n",
    "print(\"*\"*10)\n",
    "print(\"Sampled\")\n",
    "evaluate_perf(test_sampled, batch_size=16, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "Sef987fwocQp",
    "XfTt9TkM7_3i"
   ],
   "machine_shape": "hm",
   "name": "symphonia_pitchnet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
